{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b730d2-40c1-4991-ba20-532868ae8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelDefinition:\n",
    "    dataset: Dataset\n",
    "    dataset_args: dict\n",
    "    split_dataset: bool\n",
    "    iterator: GameBatchIterator\n",
    "    image_loader: ImageLoader\n",
    "    bounding_box_loader: ImageLoader\n",
    "    sender: Module\n",
    "    sender_args: dict\n",
    "    receiver: Module\n",
    "    receiver_args: dict\n",
    "    loss_function: Callable\n",
    "\n",
    "\n",
    "class CoordinatePredictorGameDataset(CoordinatePredictorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.file, \"r\") as f:\n",
    "            return CoordinatePredictorSample(\n",
    "                image_id=str(f[\"image_id\"][index], \"utf-8\"),\n",
    "                image=load_tensor(f[\"image\"][index]),\n",
    "                target_pixels=load_tensor(f[\"target_pixels\"][index]),\n",
    "                target_region=load_tensor(f[\"target_region\"][index]),\n",
    "                attribute_tensor=load_tensor(f[\"attribute_tensor\"][index]),\n",
    "                locations=load_tensor(f[\"locations\"][index]),\n",
    "                masked_image=load_tensor(f[\"masked_image\"][index]),\n",
    "                bounding_boxes=load_tensor(f[\"bounding_boxes\"][index]),\n",
    "            )\n",
    "\n",
    "@dataclass\n",
    "class CoordinatePredictorSample:\n",
    "    image_id: str\n",
    "    image: torch.Tensor\n",
    "\n",
    "    # target\n",
    "    target_pixels: torch.Tensor\n",
    "    target_region: torch.Tensor\n",
    "\n",
    "    # addtional (optional) information\n",
    "    attribute_tensor: torch.Tensor = torch.tensor(0)\n",
    "    locations: torch.Tensor = torch.tensor(0)\n",
    "    masked_image: torch.Tensor = torch.tensor(0)\n",
    "    bounding_boxes: torch.Tensor = torch.tensor(0)\n",
    "\n",
    "\n",
    "def load_tensor(data):\n",
    "    match type(data):\n",
    "        case torch.Tensor:\n",
    "            return data\n",
    "        case numpy.ndarray:\n",
    "            return torch.from_numpy(data)\n",
    "        case builtins.list:\n",
    "            match type(data[0]):\n",
    "                case builtins.str:\n",
    "                    return data\n",
    "                case _:\n",
    "                    return torch.stack([load_tensor(d) for d in data])\n",
    "        case _:\n",
    "            return torch.tensor(data)\n",
    "\n",
    "class SingleObjectImageMasker(ImageMasker):\n",
    "    def get_masked_image(self, image, scene, target_object):\n",
    "        masked_image = image.copy()\n",
    "        MASK_SIZE = masked_image.size[0] / 10\n",
    "        x_center, y_center, _ = scene[\"objects\"][target_object][\"pixel_coords\"]\n",
    "        pixels = masked_image.load()\n",
    "\n",
    "        for i, j in itertools.product(\n",
    "            range(masked_image.size[0]), range(masked_image.size[1])\n",
    "        ):\n",
    "            if (\n",
    "                i < x_center - MASK_SIZE\n",
    "                or i > x_center + MASK_SIZE\n",
    "                or j < y_center - MASK_SIZE\n",
    "                or j > y_center + MASK_SIZE\n",
    "            ):\n",
    "                pixels[i, j] = (0, 0, 0)\n",
    "            else:\n",
    "                pixels[i, j] = (255, 255, 255)\n",
    "\n",
    "        return masked_image\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "class FeatureImageLoader(ImageLoader):\n",
    "    def __init__(self, feature_file, image_dir) -> None:\n",
    "        self.image_dir = image_dir\n",
    "        self.feature_file = feature_file\n",
    "\n",
    "        with h5py.File(feature_file, \"r\") as f:\n",
    "            feature_data_set = f[\"features\"]\n",
    "            self.image_size = feature_data_set.attrs[\"image_size\"]\n",
    "\n",
    "    def get_image(self, image_id):\n",
    "        image_index = int(image_id[-6:])\n",
    "\n",
    "        with h5py.File(self.feature_file, \"r\") as f:\n",
    "            feature_data_set = f[\"features\"]\n",
    "            features = feature_data_set[image_index]\n",
    "\n",
    "        image = Image.open(os.path.join(self.image_dir, image_id + \".png\")).convert(\n",
    "            \"RGB\"\n",
    "        )\n",
    "\n",
    "        return image, torch.from_numpy(features), self.image_size\n",
    "\n",
    "class AttentionPredictorGameBatchIterator(GameBatchIterator):\n",
    "    def __init__(self, loader, batch_size, n_batches, train_mode, seed) -> None:\n",
    "        self.loader = loader\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = n_batches\n",
    "        self.batches_generated = 0\n",
    "        self.train_mode = train_mode\n",
    "        self.random_seed = random.Random(seed)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batches_generated > self.n_batches:\n",
    "            raise StopIteration()\n",
    "\n",
    "        batch_data = self.get_batch()\n",
    "        self.batches_generated += 1\n",
    "        return batch_data\n",
    "\n",
    "    def get_batch(self):\n",
    "        sampled_indices = self.random_seed.sample(\n",
    "            range(len(self.loader.dataset)), self.batch_size\n",
    "        )\n",
    "        samples: list[CoordinatePredictorSample] = [\n",
    "            self.loader.dataset[i] for i in sampled_indices\n",
    "        ]\n",
    "\n",
    "        sender_inputs = []\n",
    "        target_regions = []\n",
    "        receiver_inputs = []\n",
    "        masked_images = []\n",
    "        attibute_tensors = []\n",
    "        image_ids = []\n",
    "\n",
    "        for sample in samples:\n",
    "            sender_inputs.append(sample.image)\n",
    "            target_regions.append(sample.target_region)\n",
    "\n",
    "            receiver_inputs.append(sample.image)\n",
    "            masked_images.append(sample.masked_image)\n",
    "            attibute_tensors.append(sample.attribute_tensor)\n",
    "            image_ids.append(int(sample.image_id[-6:]))\n",
    "\n",
    "        return (\n",
    "            torch.stack(sender_inputs),\n",
    "            torch.stack(target_regions),\n",
    "            torch.stack(receiver_inputs),\n",
    "            {\n",
    "                \"masked_image\": torch.stack(masked_images),\n",
    "                \"attribute_tensor\": torch.stack(attibute_tensors),\n",
    "                \"image_id\": torch.tensor(image_ids),\n",
    "            },\n",
    "        )\n",
    "\n",
    "class MaskedCoordinatePredictorSender(nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "     - x and y coordinates of target object\n",
    "\n",
    "    Input:\n",
    "     - image\n",
    "     - attributes (shape, size, color)\n",
    "     - center coordinates of all objects\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_encoder: ImageEncoder,\n",
    "        masked_image_encoder: ImageEncoder,\n",
    "        sender_image_embedding: int,\n",
    "        sender_hidden,\n",
    "        *_args,\n",
    "        **_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        self.masked_image_encoder = masked_image_encoder\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Flatten(), nn.LazyLinear(sender_image_embedding)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.LazyLinear(sender_hidden)\n",
    "\n",
    "    def forward(self, x, aux_input):\n",
    "        image = x\n",
    "        masked_image = aux_input[\"masked_image\"]\n",
    "\n",
    "        reduced = self.image_encoder(image)\n",
    "        reduced_masked_image = self.masked_image_encoder(masked_image)\n",
    "\n",
    "        concatenated = torch.cat(\n",
    "            (reduced, reduced_masked_image),\n",
    "            dim=1,\n",
    "        )\n",
    "        reduced = self.reduction(concatenated)\n",
    "\n",
    "        hidden = self.linear(reduced)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "\"masked_attention_predictor\": ModelDefinition(\n",
    "        dataset=CoordinatePredictorGameDataset,\n",
    "        dataset_args={\n",
    "            \"image_masker\": SingleObjectImageMasker(),\n",
    "            \"number_regions\": 14,\n",
    "        },\n",
    "        split_dataset=False,\n",
    "        image_loader=FeatureImageLoader,\n",
    "        bounding_box_loader=None,\n",
    "        iterator=AttentionPredictorGameBatchIterator,\n",
    "        sender=MaskedCoordinatePredictorSender,\n",
    "        sender_args={\n",
    "            \"image_encoder\": ClevrImageEncoder(\n",
    "                feature_extractor=DummyFeatureExtractor(), max_pool=True\n",
    "            ),\n",
    "            \"masked_image_encoder\": ClevrImageEncoder(\n",
    "                feature_extractor=ResnetFeatureExtractor(\n",
    "                    pretrained=True,\n",
    "                    avgpool=False,\n",
    "                    fc=False,\n",
    "                    fine_tune=False,\n",
    "                    number_blocks=3,\n",
    "                ),\n",
    "                max_pool=True,\n",
    "            ),\n",
    "        },\n",
    "        receiver=AttentionPredictorReceiver,\n",
    "        receiver_args={\n",
    "            \"image_encoder\": ClevrImageEncoder(\n",
    "                feature_extractor=DummyFeatureExtractor(), max_pool=False\n",
    "            ),\n",
    "        },\n",
    "        loss_function=attention_loss,\n",
    "    ),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
